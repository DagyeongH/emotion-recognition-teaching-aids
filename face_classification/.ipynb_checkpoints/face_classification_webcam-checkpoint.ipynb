{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gDMOSmlfz9o_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 12:38:37.159132: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 데이터 확인\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dataset 만들기\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Face\n",
    "import cv2\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# Model\n",
    "# import torch\n",
    "from keras.models import Sequential\n",
    "# from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "# from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "# from keras.layers.normalization import batch_normalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19946,
     "status": "ok",
     "timestamp": 1700149624247,
     "user": {
      "displayName": "최기원",
      "userId": "01859623484991989729"
     },
     "user_tz": -540
    },
    "id": "qBYTegwi0Bso",
    "outputId": "9a70a3dc-ea68-4952-b022-3e22214ed35a"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1700149641861,
     "user": {
      "displayName": "최기원",
      "userId": "01859623484991989729"
     },
     "user_tz": -540
    },
    "id": "HEpVozIJ0HeG",
    "outputId": "9b46de38-18b2-41b3-913c-622ba6afcc4b"
   },
   "outputs": [],
   "source": [
    "# cd /content/drive/MyDrive/last/im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JXEqs-qo0TJL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hongdagyeong/Documents/Pd/project_face_classification/mini_project/face_classification'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1700149661549,
     "user": {
      "displayName": "최기원",
      "userId": "01859623484991989729"
     },
     "user_tz": -540
    },
    "id": "fXfZP9HS0M0h"
   },
   "outputs": [],
   "source": [
    "shape_x = 48\n",
    "shape_y = 48\n",
    "\n",
    "# 전체 이미지에서 얼굴을 찾아내는 함수\n",
    "def detect_face(frame):\n",
    "\n",
    "    # cascade pre-trained 모델 불러오기\n",
    "    # face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # RGB를 gray scale로 바꾸기\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # cascade 멀티스케일 분류\n",
    "    detected_faces = face_cascade.detectMultiScale(gray,\n",
    "                                                   scaleFactor = 1.1,\n",
    "                                                   minNeighbors = 6,\n",
    "                                                   minSize = (shape_x, shape_y),\n",
    "                                                   flags = cv2.CASCADE_SCALE_IMAGE\n",
    "                                                  )\n",
    "\n",
    "    coord = []\n",
    "    for x, y, w, h in detected_faces:\n",
    "        if w > 100:\n",
    "            sub_img = frame[y:y+h, x:x+w]\n",
    "            coord.append([x, y, w, h])\n",
    "\n",
    "    return gray, detected_faces, coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 979,
     "status": "ok",
     "timestamp": 1700149695570,
     "user": {
      "displayName": "최기원",
      "userId": "01859623484991989729"
     },
     "user_tz": -540
    },
    "id": "UBed1vG-0R0P"
   },
   "outputs": [],
   "source": [
    "# 전체 이미지에서 찾아낸 얼굴을 추출하는 함수\n",
    "def extract_face_features(gray, detected_faces, coord, offset_coefficients=(0.075, 0.05)):\n",
    "    new_face = []\n",
    "    for det in detected_faces:\n",
    "\n",
    "        # 얼굴로 감지된 영역\n",
    "        x, y, w, h = det\n",
    "\n",
    "        # 이미지 경계값 받기\n",
    "        horizontal_offset = np.int(np.floor(offset_coefficients[0] * w))\n",
    "        vertical_offset = np.int(np.floor(offset_coefficients[1] * h))\n",
    "\n",
    "        # gray scacle 에서 해당 위치 가져오기\n",
    "        extracted_face = gray[y+vertical_offset:y+h, x+horizontal_offset:x-horizontal_offset+w]\n",
    "\n",
    "        # 얼굴 이미지만 확대\n",
    "        new_extracted_face = zoom(extracted_face, (shape_x/extracted_face.shape[0], shape_y/extracted_face.shape[1]))\n",
    "        new_extracted_face = new_extracted_face.astype(np.float32)\n",
    "        new_extracted_face /= float(new_extracted_face.max()) # sacled\n",
    "        new_face.append(new_extracted_face)\n",
    "\n",
    "    return new_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 798,
     "status": "ok",
     "timestamp": 1700149867378,
     "user": {
      "displayName": "최기원",
      "userId": "01859623484991989729"
     },
     "user_tz": -540
    },
    "id": "kieUIWCv09G7"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 375,
     "status": "ok",
     "timestamp": 1700149869982,
     "user": {
      "displayName": "최기원",
      "userId": "01859623484991989729"
     },
     "user_tz": -540
    },
    "id": "gmmg865R0Y2m"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 12:38:40.437265: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-17 12:38:40.437326: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "model = keras.models.load_model('./model/model.h5')\n",
    "\n",
    "# 인덱스번호로 웹캠연결 대부분 시스템적으로 0번부터 부여됨\n",
    "video_capture = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "error",
     "timestamp": 1700149891810,
     "user": {
      "displayName": "최기원",
      "userId": "01859623484991989729"
     },
     "user_tz": -540
    },
    "id": "vBVwZECg0SlM",
    "outputId": "19f2966d-ec9e-4be6-8cf5-26d0eb28ac92",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 12:38:45.494343: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# ret: 비디오를 성공적으로 읽어왔는지 확인 True/False\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# frame: 각 픽셀의 색상을 포함한 프레임 정보 Numpy\u001b[39;00m\n\u001b[1;32m      8\u001b[0m face_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 9\u001b[0m gray, detected_faces, coord \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# ret, frame = video_capture.read(-1)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# frame = cv2.VideoCapture(1)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(frame)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ret: 비디오를 성공적으로 읽어왔는지 확인 True/False\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# frame: 각 픽셀의 색상을 포함한 프레임 정보 Numpy\u001b[39;00m\n\u001b[1;32m     16\u001b[0m face_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m, in \u001b[0;36mdetect_face\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m     12\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# cascade 멀티스케일 분류\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m detected_faces \u001b[38;5;241m=\u001b[39m \u001b[43mface_cascade\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mscaleFactor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mminNeighbors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mminSize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCASCADE_SCALE_IMAGE\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m coord \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y, w, h \u001b[38;5;129;01min\u001b[39;00m detected_faces:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 프레임 단위로 영상 캡쳐\n",
    "while True:\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    ret, frame = video_capture.read()\n",
    "    # ret: 비디오를 성공적으로 읽어왔는지 확인 True/False\n",
    "    # frame: 각 픽셀의 색상을 포함한 프레임 정보 Numpy\n",
    "\n",
    "    face_index = 0\n",
    "    gray, detected_faces, coord = detect_face(frame)\n",
    "\n",
    "    try:\n",
    "        face_zoom = extract_face_features(gray, detected_faces, coord)\n",
    "        face_zoom = np.reshape(face_zoom[0].flatten(), (1, 48, 48, 1))\n",
    "        x, y, w, h = coord[face_index]\n",
    "\n",
    "        # 머리 둘레에 직사각형 그리기: (0, 255, 0)을 통해 녹색으로 선두께는 2\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        # 감정 예측\n",
    "        pred = model.predict(face_zoom)\n",
    "        pred_result = np.argmax(pred)\n",
    "\n",
    "        # 각 라벨별 예측 정도 표시\n",
    "        cv2.putText(frame,                                   # 텍스트를 표시할 프레임\n",
    "                    \"Angry: \" + str(round(pred[0][0], 3)),   # 텍스트 표시 \"감정: 예측 probablity\", 소수점 아래 3자리\n",
    "                    (10, 50),                                # 텍스트 위치\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,                # 폰트 종류\n",
    "                    1,                                       # 폰트 사이즈\n",
    "                    (0, 0, 255),                             # 폰트 색상\n",
    "                    2                                        # 폰트 두께\n",
    "                   )\n",
    "        cv2.putText(frame, \"Disgust: \" + str(round(pred[0][1], 3)), (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, \"Fear: \" + str(round(pred[0][2], 3)), (10, 130), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, \"Happy: \" + str(round(pred[0][3], 3)), (10, 170), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, \"Sad: \" + str(round(pred[0][4], 3)), (10, 210), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, \"Surprise: \" + str(round(pred[0][5], 3)), (10, 250), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, \"Neutral: \" + str(round(pred[0][6], 3)), (10, 290), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # 예측값이 높은 라벨 하나만 프레임 옆에 표시\n",
    "        if pred_result == 0:\n",
    "            cv2.putText(frame, \"Angry \" + str(round(pred[0][0], 2)), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 5)\n",
    "        elif pred_result == 1:\n",
    "            cv2.putText(frame, \"Disgust \" + str(round(pred[0][1], 2)), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 5)\n",
    "        elif pred_result == 2:\n",
    "            cv2.putText(frame, \"Fear \" + str(round(pred[0][2], 2)), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 5)\n",
    "        elif pred_result == 3:\n",
    "            cv2.putText(frame, \"Happy \" + str(round(pred[0][3], 2)), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 5)\n",
    "        elif pred_result == 4:\n",
    "            cv2.putText(frame, \"Sad \" + str(round(pred[0][4], 2)), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 5)\n",
    "        elif pred_result == 5:\n",
    "            cv2.putText(frame, \"Surprise \" + str(round(pred[0][5], 2)), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 5)\n",
    "        else:\n",
    "            cv2.putText(frame, \"Neutral \" + str(round(pred[0][6], 2)), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 5)\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # 결과 표시\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # 사용자가 q 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 웹캠 해지\n",
    "video_capture.release()\n",
    "\n",
    "# 창 닫기: 창이 안닫히는 경우 쥬피터 닫기\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMS2BvAw9GwAEID6UQ5xneu",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
